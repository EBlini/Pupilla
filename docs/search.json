[{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Pupilla_TOBII_ReduceFeatures","text":"Pupilla gathers several functions designed facilitate analysis pupillometry experiments commonly performed cognitive neuroscience, e.g. event-related designs, although use much general. typical analysis pipeline , coarsely, include following steps: Read data. part can vary lot depending eyetracker used, individual OS, local paths, experiment coded, etc. Pupilla provide utility functions read common eyetrackers (e.g., TOBII, EyeLink) clearly passage need tailored files. Prepare data. , part may need tailored specific needs; however, several steps common across pipelines, presented vignette. Preprocessing. Pupillometry needs robust preprocessing raw data, order reduce noise artifacts (due blinks). data properly prepared, aspect can translated across several different scenarios. course, flexibility adapting data warmly advised. Statistical modelling. Pupilla offers two approaches: 1) crossvalidated LMEMs Mathôt & Vilotijević, 2022); 2) original approach feature reduction. vignette covers illustrates second option. example use data Blini Zorzi, 2023. Data can retrieved associated OSF repository. eyetracker used TOBII spectrum. Unfortunately, eyetracking data acquired way quite large, meaning reading take time. study (termed Passive Viewing (PV) task) 40 participants (following exclusions), 20 smokers 20 non smokers, given, name suggests, several images look : related nicotine, neutral controls. Contrary name suggest, instead, also report (hence somehow actively) occurrence rare probe, presented screen sparingly; ensure central fixation maintained together minimum task engagement. Details provided accompanying paper, matters smokers found pupillary constriction nicotine-related images (opposed neutral ones) presented. Please note pipeline default parameters Pupilla changed since paper came PBR, results slightly different.","code":""},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"read-the-data","dir":"Articles","previous_headings":"","what":"Read the data","title":"Pupilla_TOBII_ReduceFeatures","text":"library Pupilla must installed first, , devtools: Dependencies installed automatically. need load following packages: following steps section vary lot function software used presentation stimuli machine. Pupilla tested windows machines, may thus troubles using utility functions read participants altogether. Windows: can see: working directory set - change !; 51 subjects corresponding 102 files default OpenSesame splits eyetracker behavioral files. , reading files straightforward. utility function work , however, just assume works iterating data.table::fread() across (eyetracking) files. Also, please note default first 7 lines skipped, eyetracker files may need different values!","code":"#install.packages(\"devtools\") devtools::install_github(\"EBlini/Pupilla\") library(\"Pupilla\") library(\"dplyr\") ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union library(\"ggplot2\")  library(\"tidyr\")  options(dplyr.summarise.inform = FALSE) #set your own working directory first! #wd= choose.dir()  subject= 1:51 #vector of ids #groups- whether ids are smokers or not;  #this I didn't know beforehand so I have to add manually this var group= c(\"NS\", \"S\", \"NS\", \"NS\", \"S\",\"S\", \"NS\", \"S\",          \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"NS\", \"S\",          \"NS\", \"S\", \"NS\", \"S\", \"NS\", \"NS\", \"S\", \"NS\",          \"S\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"S\",          \"S\", \"S\", \"S\", \"NS\", \"NS\", \"NS\", \"NS\", \"S\",          \"NS\", \"NS\", \"NS\", \"S\", \"S\", \"S\", \"S\", \"S\",          \"S\", \"S\", \"S\")  #read all the files data= read_TOBII(subject, wd)  #split for eyetracker and behavioral data ET= data$ET BD= data$BD"},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"prepare-the-data","dir":"Articles","previous_headings":"","what":"Prepare the data","title":"Pupilla_TOBII_ReduceFeatures","text":"bad (?) habit record essential info eyetracker file, :) result, often variables present behavioral file (e.g., response time, condition) must copied eyetracker file, different dimensions (several lines per trial, depending sampling rate). Pupilla utility functions precisely . Let’s move order though. start filling “Event” column, blank except Event changes: Based Event column changing value, can establish trial number (yes, info also missing eyetracker file!). detect_change() simply updates counter every instance parameter “key” appears vector (first time). initial samples assigned trial, shall removed: can start now copying relevant variables ET dataframe. start adding variable Phase (whether trial labelled practice, therefore removed afterwards, experimental). move variable Trial: finally variables make experimental design: (, Condition relevant , rest can skip vignette). can finally start handling preparing signal pupil size! TOBII acquired left right eye. consolidate two one single variable represents average two eyes - judged valid TOBII’s algorithms. TOBII stores pupil size mms, can fetch plausible values (2 7 mms) discard outlier ones straight away. isolate two experimental stages: scrambled images (baseline) vs target images. can now realign timestamps first sample scrambled phase. can see timestamps absolute values, difference really constant. theory trial last 4500 ms, one two reasons missed something last longer, shall discard . Now everything clean, realign Time beginning trial, moment target presented: Finally, moment Group variable added. Furthermore, discard participants present sufficient valid trials; actually seen afterwards, experiment add another task eye movements quality important, exclusions decided based results tasks.","code":"ET$Event= ifelse(ET$Event== \"\", NA, ET$Event) ET= tidyr::fill(ET, Event, .direction = \"down\") ET$Subject= ET$p_ID ET= ET %>%   group_by(Subject) %>%   mutate(Trial= detect_change(Event,                                key= \"scrambled\")) ET= ET[ET$Trial>=0,] #whether it's practice or experiment ET$Phase= copy_variable(\"Phase\") #discard practice ET= ET[ET$Phase== \"experiment\",] BD= BD[BD$Phase== \"experiment\",] ET$Trial= copy_variable(\"Trial\")  range(ET$Trial) ## [1]   0 199 range(BD$Trial) ## [1]   0 199 ET$Condition= copy_variable(\"Condition\") # ET$Cue= copy_variable(\"Cue\") # ET$Accuracy= copy_variable(\"Accuracy\") # ET$Image= copy_variable(\"Image\") # ET$RT = as.numeric(copy_variable(\"RT\")) ET$Pupil= consolidate_signal(ET$PupilSizeLeft, ET$PupilSizeRight,                              ET$PupilValidityLeft, ET$PupilValidityRight,                              strategy = \"conservative\",                              plausible= c(2, 7)) ET= ET[ET$Event %in% c(\"scrambled\", \"target\"),] head(ET$TimeStamp) ## [1] 58675.74 58676.48 58686.21 58686.89 58687.61 58687.98 ET= ET %>%   group_by(Subject, Trial) %>%   mutate(Time= c(0,                  cumsum(diff(TimeStamp))))  head(ET$Time) ## [1]  0.000  0.740 10.469 11.155 11.874 12.240 range(ET$Time) ## [1]     0.00 18578.32 ET= ET %>%   group_by(Subject, Trial) %>%   mutate(Anomaly= ifelse(max(Time)>4500, 1, 0))  (table(ET$p_ID[ET$Anomaly== 1],        ET$Trial[ET$Anomaly== 1])) #for 1 participant, the trial around the break... ##      ##        100 ##   15 11237 ET= ET[ET$Anomaly== 0,] ET= ET %>%   group_by(Subject, Trial) %>%   mutate(Time= Time - Time[Event== \"target\"][1])  ET= ET[ET$Time >-1000 & ET$Time<3000,] ET= ET %>% filter(!Subject %in% c(2, 9, 10, 13, 25, 29, 30, 34, 38, 48, 49))  #assign group ET$Group= group[ET$Subject]"},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"preprocessing","dir":"Articles","previous_headings":"","what":"Preprocessing","title":"Pupilla_TOBII_ReduceFeatures","text":"can finally move real thing! signal must processed impact artifacts, blinks, etc. reduced. easiest way Pupilla use pre_process() function. may want, however, consider whether specific default parameters applicable data. description parameters, see ?pp_options(). can always change default parameters calling options globally, within function . E.g.: ’s ! can check result pipeline visually follows:  can use function Pupilla::check_all_series() plots (, ids trials) saved images path. image, black dots represent raw, initial data. red line depicts instead reconstructed, preprocessed signal. pre_process() simply runs, order, functions deblinking (velocity-based criterion), interpolation, smoothing cubic splines. Trials data points reach given quality threshold set NA; trials can recovered , instead, recovered. , trials couldn’t restore reliable signal, simply discard ! Another common step downsampling, choose bins 25 ms: Next, personally prefer work z-scores instead arbitrary units mms, standardized measure. : last, crucial step baseline subtraction. analogy done paper simply realign traces beginning target presentation phase, just like done Time. extended period advisable. done!","code":"#the default parameters: pp_options() ## $thresh ## [1] 3 ##  ## $speed_method ## [1] \"z\" ##  ## $extend_by ## [1] 3 ##  ## $island_size ## [1] 4 ##  ## $extend_blink ## [1] 3 ##  ## $overall_thresh ## [1] 0.4 ##  ## $consecutive_thresh ## NULL ##  ## $spar ## [1] 0.7 #this changes the width of the window for smoothing pp_options(\"spar\"= 0.8)   #entire preprocessing ET= ET %>%   group_by(Subject, Trial) %>%   mutate(Pupil_pp= pre_process(Pupil, Time)) ET %>% filter(Subject==12 & Trial== 104) %>%   check_series(\"Pupil\", \"Pupil_pp\", \"Time\") #drop ET= ET %>% filter(!is.na(Pupil_pp)) ET$Time= downsample_time(ET$Time, 25)  #summarise the data for the new binned variable ET= ET %>%   group_by(Subject, Group, Condition, Trial, Time) %>%   summarise(Pupil= median(Pupil_pp, na.rm = T)) ET= ET %>%   group_by(Subject) %>%   mutate(Pupil= (Pupil-mean(na.omit(Pupil)))/sd(na.omit(Pupil))) ET= ET %>%   group_by(Subject, Trial) %>%   mutate(Pupil= Pupil - Pupil[Time== 0][1])"},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"analysis","dir":"Articles","previous_headings":"","what":"Analysis","title":"Pupilla_TOBII_ReduceFeatures","text":"Briefly, data looks like :  now different paths statistical modelling. original paper choose cluster-based permutation test. approach often computationally-intensive, though works well. approaches involve crossvalidated LMEMs (implemented package Pupilla shown another vignette) feature reduction. Feature reduction norm pupillometry, common branches neuroimaging - e.g., fMRI. works well, data large, reducing dimensions manageable variables work . case pupillometry, signal strongly autocorrelated, particularly appealing. Pupilla can summarise traces PCA ICA follows.","code":"## $ncol ##  ## $nrow ## NULL ##  ## $byrow ## NULL ##  ## $widths ## NULL ##  ## $heights ## NULL ##  ## $guides ## NULL ##  ## $tag_level ## NULL ##  ## $design ## NULL ##  ## attr(,\"class\") ## [1] \"plot_layout\""},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"pca","dir":"Articles","previous_headings":"Analysis","what":"PCA","title":"Pupilla_TOBII_ReduceFeatures","text":"traces 40 participants x () 200 trials can summarised PCs (need 3 variables account >98% data!): PC accounts specific share variance, distinctive loadings - can think weighted contribution PC time point, way similar cluster, though graded. can assess loadings directly plot conveniently returned plot_loadings().  loadings first PC, expected, resemble much shape data. trial steady pupil dilation, well captured later timepoints larger weights. sign loadings , instead, arbitrary, well multiply -1. component summarised one score per trial! far manageable uses, e.g. obtain intuitive easy interpret summary scores. Scores can used directly - e.g., correlate experimental variables questionnaires neuroimaging data - used second level analysis (e.g., simple t tests). case Group x Condition interaction, start summarising trial summary scores, scoring difference conditions: PC1:  significant interaction group condition captured first PC! second PC , instead, significant: Features first ones progressively account remaining variance, may thus accomodate subtle differences conditions alter necessarily overall shape pupillary dilation. words, next pcs describe sort contrast functions like one:","code":"data= ET[ET$Time>0,] #remove the baseline dv= \"Pupil\" time= \"Time\" id= \"Subject\" trial= \"Trial\" add= c(\"Group\", \"Condition\") #save to final dataframe Ncomp= NULL #defaults to 95% of variance retained  rf = reduce_PCA(data,                 dv,                 time,                 id,                 trial,                 Ncomp = NULL,                 scaling = F,                 add) rf$summaryPCA[, 1:4] ##                             PC1      PC2      PC3       PC4 ## Standard deviation     5.747356 1.889998 1.108391 0.7121974 ## Proportion of Variance 0.849950 0.091910 0.031610 0.0130500 ## Cumulative Proportion  0.849950 0.941860 0.973480 0.9865300 plot_loadings(\"PC1\", rf) Scores= rf$Scores  Scores= Scores %>%    group_by(id, Group, Condition) %>%    summarise(PC1= mean(PC1), PC2= mean(PC2)) %>%    group_by(id, Group) %>%    reframe(PC1= PC1[Condition== \"Control\"]-PC1[Condition== \"Nicotine-related\"],            PC2= PC2[Condition== \"Control\"]-PC2[Condition== \"Nicotine-related\"]) #plots of the difference ggplot(Scores, aes(x= Group,                    color= Group,                    y= PC1)) +   geom_point(position = position_dodge2(0.3)) t.test(Scores$PC1[Scores$Group== \"Smokers\"],        Scores$PC1[Scores$Group== \"Non smokers\"]) ##  ##  Welch Two Sample t-test ##  ## data:  Scores$PC1[Scores$Group == \"Smokers\"] and Scores$PC1[Scores$Group == \"Non smokers\"] ## t = 2.303, df = 36.918, p-value = 0.02701 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  0.07770775 1.21602944 ## sample estimates: ##  mean of x  mean of y  ##  0.3538250 -0.2930436 t.test(Scores$PC2[Scores$Group== \"Smokers\"],        Scores$PC2[Scores$Group== \"Non smokers\"]) ##  ##  Welch Two Sample t-test ##  ## data:  Scores$PC2[Scores$Group == \"Smokers\"] and Scores$PC2[Scores$Group == \"Non smokers\"] ## t = -0.57373, df = 34.168, p-value = 0.5699 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  -0.3254013  0.1821000 ## sample estimates: ##   mean of x   mean of y  ## -0.05093052  0.02072016 plot_loadings(\"PC2\", rf)"},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"ica","dir":"Articles","previous_headings":"Analysis","what":"ICA","title":"Pupilla_TOBII_ReduceFeatures","text":"Choosing ICA simple ! Pupilla uses ica::icafast independent components analysis. overall explained variance remains PCA. However, single contribution components weighted : words, trust much title loadings plot - refers PCA model:  case first IC first PC similar, scale changes bit, reflect overall dilation trend - , sign loadings really matter (just check direction interpretation data). Results (somehow) similar PCA:","code":"rf2 = reduce_ICA(data,                  dv,                  time,                  id,                  trial,                  Ncomp = NULL,                  scaling = F,                  add) rf2$ICA$vafs ## [1] 0.72918272 0.17781761 0.06647561 plot_loadings(\"IC1\", rf2) Scores2= rf2$Scores  Scores2= Scores2 %>%    group_by(id, Group, Condition) %>%    summarise(IC1= mean(IC1), IC2= mean(IC2)) %>%    group_by(id, Group) %>%    reframe(IC1= IC1[Condition== \"Control\"]-IC1[Condition== \"Nicotine-related\"],            IC2= IC2[Condition== \"Control\"]-IC2[Condition== \"Nicotine-related\"]) #plots of the difference ggplot(Scores2, aes(x= Group,                    color= Group,                    y= IC1)) +   geom_point(position = position_dodge2(0.3)) t.test(Scores2$IC1[Scores$Group== \"Smokers\"],        Scores2$IC1[Scores$Group== \"Non smokers\"]) ##  ##  Welch Two Sample t-test ##  ## data:  Scores2$IC1[Scores$Group == \"Smokers\"] and Scores2$IC1[Scores$Group == \"Non smokers\"] ## t = -1.9344, df = 37.973, p-value = 0.06054 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  -0.215057273  0.004892434 ## sample estimates: ##   mean of x   mean of y  ## -0.05883640  0.04624602"},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Pupilla_TOBII_ReduceFeatures","text":"vignette went example use functions Pupilla data, areas encompassing loading, preparing, preprocessing data. Furthermore, novel approach - , , object active research - analyze data presented. approach signal decomposed , manageable scores, explain efficiently (severely autocorrelated) data handful scores. scores can attributed differences pupil size different time points, can explored visually loadings specific components. Another advantage , case multiple components presenting significant effects, weights can backprojected linear combination coefficients loadings (see, details, backprojection package, FCnet). approach therefore potential flexible. approach discussed details accompanying paper.","code":""},{"path":"/articles/Pupilla_TOBII_ReduceFeatures.html","id":"appendix","dir":"Articles","previous_headings":"","what":"Appendix","title":"Pupilla_TOBII_ReduceFeatures","text":"Packages’ versions:","code":"sessionInfo() ## R version 4.2.3 (2023-03-15 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ##  ## Matrix products: default ##  ## locale: ## [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8    LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C                   LC_TIME=Italian_Italy.utf8     ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] tidyr_1.3.0        ggplot2_3.4.1      dplyr_1.1.0        Pupilla_0.0.0.9000 ##  ## loaded via a namespace (and not attached): ##  [1] zoo_1.8-11        settings_0.2.7    tidyselect_1.2.0  xfun_0.39         bslib_0.4.2       purrr_1.0.1       ica_1.0-3         lattice_0.20-45   colorspace_2.1-0  vctrs_0.6.0       generics_0.1.3    htmltools_0.5.4   yaml_2.3.7        utf8_1.2.3        rlang_1.1.0       pkgdown_2.0.7     jquerylib_0.1.4   pillar_1.8.1      glue_1.6.2        withr_2.5.0       lifecycle_1.0.3   stringr_1.5.0     munsell_0.5.0     gtable_0.3.1      ragg_1.2.5        memoise_2.0.1     evaluate_0.20     labeling_0.4.2    knitr_1.42        fastmap_1.1.1     fansi_1.0.4       highr_0.10        scales_1.2.1      cachem_1.0.7      desc_1.4.2        jsonlite_1.8.4    farver_2.1.1      systemfonts_1.0.4 fs_1.6.1          textshaping_0.3.6 digest_0.6.31     stringi_1.7.12    rprojroot_2.0.3   grid_4.2.3        here_1.0.1        cli_3.6.0         tools_4.2.3       magrittr_2.0.3    sass_0.4.5        patchwork_1.1.2   tibble_3.2.0      pkgconfig_2.0.3   data.table_1.14.8 rmarkdown_2.20    rstudioapi_0.14   ## [56] R6_2.5.1          compiler_4.2.3"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Elvio Blini. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Blini E (2024). Pupilla: Process Analyze Eye-tracking Pupillometry Data. R package version 0.0.0.9000, https://github.com/EBlini/Pupilla.","code":"@Manual{,   title = {Pupilla: Process and Analyze Eye-tracking and Pupillometry Data},   author = {Elvio Blini},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://github.com/EBlini/Pupilla}, }"},{"path":"/index.html","id":"pupilla","dir":"","previous_headings":"","what":"Process and Analyze Eye-tracking and Pupillometry Data","title":"Process and Analyze Eye-tracking and Pupillometry Data","text":"R package processing analysis eye-tracking pupillometry data.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Process and Analyze Eye-tracking and Pupillometry Data","text":"package currently available GitHub. installation requires R package devtools.","code":"# install.packages(\"devtools\") devtools::install_github(\"EBlini/Pupilla\")"},{"path":"/index.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Process and Analyze Eye-tracking and Pupillometry Data","text":"collection functions wrappers useful preprocessing analyze eyetracking data, special focus pupillometry. functions designed data TOBII’s eyetrackers collected third-part experiment builders OpenSesame. functions considered development: requests suggestions welcome, :) . useful references, example results functions, see Blini Zorzi, 2023.","code":""},{"path":"/reference/check_all_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Convenience function to check series across IDs and Trials, and save a plot in the current path — check_all_series","title":"Convenience function to check series across IDs and Trials, and save a plot in the current path — check_all_series","text":"Convenience function check series across IDs Trials, saves multiple plots current path creating folders subfolders hopefully meaningful way. Note massive data take time, may want debug first subset data. defaults 'ggsave', used internally, now bit stiff may become flexible future. 'check_series' function invoked plots, thus may want check relative help page.","code":""},{"path":"/reference/check_all_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convenience function to check series across IDs and Trials, and save a plot in the current path — check_all_series","text":"","code":"check_all_series(data, ID, Trial, series1, series2, time)"},{"path":"/reference/check_all_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convenience function to check series across IDs and Trials, and save a plot in the current path — check_all_series","text":"data Mandatory, differently 'check_series'. IDs Trials levels around loop set retrieved . ID string indicating name ID column. Trial string indicating name Trial column. series1 Unlike 'check_series', must string indicating name first time series plot. series2 Unlike 'check_series', must string indicating name second time series plot. time Unlike 'check_series', must string indicating elapsed time, used x-axis.","code":""},{"path":"/reference/check_all_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convenience function to check series across IDs and Trials, and save a plot in the current path — check_all_series","text":"plot.","code":""},{"path":"/reference/check_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots two time series against each other — check_series","title":"Plots two time series against each other — check_series","text":"function plots two time series . intended use generally check original vs. preprocessed data. first series plotted black line, second one - typically reconstructed series - red line. Nas show interruptions lines. function can used within dplyr's style pipes - case 'data' can omitted variables must provided quoted variables' names - standard vectors may provided - case 'data' NULL args passed name.","code":""},{"path":"/reference/check_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots two time series against each other — check_series","text":"","code":"check_series(data, series1, series2, time)"},{"path":"/reference/check_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots two time series against each other — check_series","text":"data Optional. Can omitted passed dplyr's style pipelines ('.'), case arguments passed quoted variables' names. series1 vector variable values first time series. plotted means black line. Typically, original data. series2 vector variable values second time series. plotted means red line. Typically, processed data. time vector variable indicating elapsed time, used x axis.","code":""},{"path":"/reference/check_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots two time series against each other — check_series","text":"plot.","code":""},{"path":"/reference/consolidate_signal.html","id":null,"dir":"Reference","previous_headings":"","what":"Consolidate pupil data according to different heuristics — consolidate_signal","title":"Consolidate pupil data according to different heuristics — consolidate_signal","text":"Mostly used, e.g., pupil data available eyes one needs single variable. Results weighted vectors signal validity provided eye-trackers. absent, signals valid assumed.","code":""},{"path":"/reference/consolidate_signal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consolidate pupil data according to different heuristics — consolidate_signal","text":"","code":"consolidate_signal(   s1,   s2,   v1,   v2,   strategy = c(\"conservative\", \"liberal\", \"pick_best\"),   plausible = NULL )"},{"path":"/reference/consolidate_signal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consolidate pupil data according to different heuristics — consolidate_signal","text":"s1 vector first signal. s2 vector second signal. v1 vector weights first signal. v2 vector weights second signal. strategy strategy mixing two signals. Conservative takes mean signals valid. Liberal additionally take one valid signal even valid. Pick_best chooses best overall signal (valid often) disregard one two valid. plausible vector length 2 defining range plausible values pupil size. provided, values outside range set NA.","code":""},{"path":"/reference/consolidate_signal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consolidate pupil data according to different heuristics — consolidate_signal","text":"numeric vector consolidated pupil size NAs available.","code":""},{"path":"/reference/copy_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"copy a variable from one data.frame to another of different length given ID and Trial constraints — copy_variable","title":"copy a variable from one data.frame to another of different length given ID and Trial constraints — copy_variable","text":"Sometimes info relevant eye-tracking file found associated behavioral file. data.frames different dimensions, thus copying one variable another can cumbersome. function job expanding relevant information accordingly exploits constraints two files. task performed ID separately requires \"Trial\" variable used calculate amount required expansion data.","code":""},{"path":"/reference/copy_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"copy a variable from one data.frame to another of different length given ID and Trial constraints — copy_variable","text":"","code":"copy_variable(   var_name,   id_var = \"p_ID\",   constrained_var = \"Trial\",   larger_df = ET,   smaller_df = BD )"},{"path":"/reference/copy_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"copy a variable from one data.frame to another of different length given ID and Trial constraints — copy_variable","text":"var_name string suggesting variable look smaller data.frame (usually behavioral one) copy larger data.frame (usually eye-tracker one). id_var name ID variable grouping variable assignment must separated (e.g., performed participant). Can NULL grouping. constrained_var name variable represents available costraint. example, can Trial number 'var_name' expanded value value Trial number. larger_df larger data.frame. output vector match number rows dataframe. Typically, eye-tracker dataframe. smaller_df smaller dataframe includes 'var_name'.","code":""},{"path":"/reference/copy_variable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"copy a variable from one data.frame to another of different length given ID and Trial constraints — copy_variable","text":"vector 'length= nrow(larger_df)'.","code":""},{"path":"/reference/decode_signal.html","id":null,"dir":"Reference","previous_headings":"","what":"Highlights and test the time-course of effects through crossvalidation\r\nThis function is meant to implement, roughly and with no assurance of full compatibility whatsoever,\r\nthe procedure proposed by Mathôt and Vilotijević (2022, Behavior Research Methods).\r\nFirst, each trial (i.e., one complete time series) is assigned to one fold in a\r\ndeterministic fashion (the first trial to the first fold, then the second trial to\r\nthe second fold, etc.). In doing that, there is no regard of how conditions are\r\ndistributed across folds, i.e. data may be slightly unbalanced; thus, you should think\r\ncarefully as to whether this strategy applies to your design (e.g., blocked conditions).\r\nThen, data are separated for each time-point, and a LMEM as specified by the 'formula'\r\nparameter, which is passed to 'lmerTest::lmer', is performed by iteratively leaving\r\none fold out. This results in a table, with as many rows as effects implied\r\nby the formula by 'nfolds', summarising which time point had a peak t-value (in absolute value)\r\nin the trained folds. In a separate table these peak values are tested: the dependent\r\nvariable becomes, for each fold, the variable provided by 'dv' at that specific peak.\r\nAnother LMEM is then computed by using this newly created variable. One problem with\r\nthis approach is that peak values can be all over the places, depending on your data.\r\nAlso, choosing the time-points based on the maximum value in the training dataset can\r\noccasionally decrease the precision of the estimate or give overfitting. You may use\r\nthis approach if you are confident that a specific effect only has an effect at a specific\r\nwindow; effects with multiple windows - e.g., an early and late impact on pupil size - may\r\nnot be properly captured with this approach. Therefore,\r\nin addition to this procedure, a very coarse consensus is seek by assessing,\r\nacross all folds and effects, which time points resulted in t-values above a certain\r\nthreshold; if the same time points pop out consistently across folds (e.g.,= 'consensus_thresh' % of the times), then the time point is retained; all time-pointsretained in the consensus are collapsed (averaged), and a final LMEM is performed withthese time points. This can be interpreted more similarly to a cluster-basedpermutation test (although it is not the same). — decode_signal","title":"Highlights and test the time-course of effects through crossvalidation\r\nThis function is meant to implement, roughly and with no assurance of full compatibility whatsoever,\r\nthe procedure proposed by Mathôt and Vilotijević (2022, Behavior Research Methods).\r\nFirst, each trial (i.e., one complete time series) is assigned to one fold in a\r\ndeterministic fashion (the first trial to the first fold, then the second trial to\r\nthe second fold, etc.). In doing that, there is no regard of how conditions are\r\ndistributed across folds, i.e. data may be slightly unbalanced; thus, you should think\r\ncarefully as to whether this strategy applies to your design (e.g., blocked conditions).\r\nThen, data are separated for each time-point, and a LMEM as specified by the 'formula'\r\nparameter, which is passed to 'lmerTest::lmer', is performed by iteratively leaving\r\none fold out. This results in a table, with as many rows as effects implied\r\nby the formula by 'nfolds', summarising which time point had a peak t-value (in absolute value)\r\nin the trained folds. In a separate table these peak values are tested: the dependent\r\nvariable becomes, for each fold, the variable provided by 'dv' at that specific peak.\r\nAnother LMEM is then computed by using this newly created variable. One problem with\r\nthis approach is that peak values can be all over the places, depending on your data.\r\nAlso, choosing the time-points based on the maximum value in the training dataset can\r\noccasionally decrease the precision of the estimate or give overfitting. You may use\r\nthis approach if you are confident that a specific effect only has an effect at a specific\r\nwindow; effects with multiple windows - e.g., an early and late impact on pupil size - may\r\nnot be properly captured with this approach. Therefore,\r\nin addition to this procedure, a very coarse consensus is seek by assessing,\r\nacross all folds and effects, which time points resulted in t-values above a certain\r\nthreshold; if the same time points pop out consistently across folds (e.g.,= 'consensus_thresh' % of the times), then the time point is retained; all time-pointsretained in the consensus are collapsed (averaged), and a final LMEM is performed withthese time points. This can be interpreted more similarly to a cluster-basedpermutation test (although it is not the same). — decode_signal","text":"Highlights test time-course effects crossvalidation function meant implement, roughly assurance full compatibility whatsoever, procedure proposed Mathôt Vilotijević (2022, Behavior Research Methods). First, trial (.e., one complete time series) assigned one fold deterministic fashion (first trial first fold, second trial second fold, etc.). , regard conditions distributed across folds, .e. data may slightly unbalanced; thus, think carefully whether strategy applies design (e.g., blocked conditions). , data separated time-point, LMEM specified 'formula' parameter, passed 'lmerTest::lmer', performed iteratively leaving one fold . results table, many rows effects implied formula 'nfolds', summarising time point peak t-value (absolute value) trained folds. separate table peak values tested: dependent variable becomes, fold, variable provided 'dv' specific peak. Another LMEM computed using newly created variable. One problem approach peak values can places, depending data. Also, choosing time-points based maximum value training dataset can occasionally decrease precision estimate give overfitting. may use approach confident specific effect effect specific window; effects multiple windows - e.g., early late impact pupil size - may properly captured approach. Therefore, addition procedure, coarse consensus seek assessing, across folds effects, time points resulted t-values certain threshold; time points pop consistently across folds (e.g.,= 'consensus_thresh' % times), time point retained; time-pointsretained consensus collapsed (averaged), final LMEM performed withthese time points. can interpreted similarly cluster-basedpermutation test (although ).","code":""},{"path":"/reference/decode_signal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Highlights and test the time-course of effects through crossvalidation\r\nThis function is meant to implement, roughly and with no assurance of full compatibility whatsoever,\r\nthe procedure proposed by Mathôt and Vilotijević (2022, Behavior Research Methods).\r\nFirst, each trial (i.e., one complete time series) is assigned to one fold in a\r\ndeterministic fashion (the first trial to the first fold, then the second trial to\r\nthe second fold, etc.). In doing that, there is no regard of how conditions are\r\ndistributed across folds, i.e. data may be slightly unbalanced; thus, you should think\r\ncarefully as to whether this strategy applies to your design (e.g., blocked conditions).\r\nThen, data are separated for each time-point, and a LMEM as specified by the 'formula'\r\nparameter, which is passed to 'lmerTest::lmer', is performed by iteratively leaving\r\none fold out. This results in a table, with as many rows as effects implied\r\nby the formula by 'nfolds', summarising which time point had a peak t-value (in absolute value)\r\nin the trained folds. In a separate table these peak values are tested: the dependent\r\nvariable becomes, for each fold, the variable provided by 'dv' at that specific peak.\r\nAnother LMEM is then computed by using this newly created variable. One problem with\r\nthis approach is that peak values can be all over the places, depending on your data.\r\nAlso, choosing the time-points based on the maximum value in the training dataset can\r\noccasionally decrease the precision of the estimate or give overfitting. You may use\r\nthis approach if you are confident that a specific effect only has an effect at a specific\r\nwindow; effects with multiple windows - e.g., an early and late impact on pupil size - may\r\nnot be properly captured with this approach. Therefore,\r\nin addition to this procedure, a very coarse consensus is seek by assessing,\r\nacross all folds and effects, which time points resulted in t-values above a certain\r\nthreshold; if the same time points pop out consistently across folds (e.g.,= 'consensus_thresh' % of the times), then the time point is retained; all time-pointsretained in the consensus are collapsed (averaged), and a final LMEM is performed withthese time points. This can be interpreted more similarly to a cluster-basedpermutation test (although it is not the same). — decode_signal","text":"","code":"decode_signal(   data,   formula,   dv,   time,   id,   trial,   nfolds = 4,   t_thresh = 2,   consensus_thresh = 0.75 )"},{"path":"/reference/decode_signal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Highlights and test the time-course of effects through crossvalidation\r\nThis function is meant to implement, roughly and with no assurance of full compatibility whatsoever,\r\nthe procedure proposed by Mathôt and Vilotijević (2022, Behavior Research Methods).\r\nFirst, each trial (i.e., one complete time series) is assigned to one fold in a\r\ndeterministic fashion (the first trial to the first fold, then the second trial to\r\nthe second fold, etc.). In doing that, there is no regard of how conditions are\r\ndistributed across folds, i.e. data may be slightly unbalanced; thus, you should think\r\ncarefully as to whether this strategy applies to your design (e.g., blocked conditions).\r\nThen, data are separated for each time-point, and a LMEM as specified by the 'formula'\r\nparameter, which is passed to 'lmerTest::lmer', is performed by iteratively leaving\r\none fold out. This results in a table, with as many rows as effects implied\r\nby the formula by 'nfolds', summarising which time point had a peak t-value (in absolute value)\r\nin the trained folds. In a separate table these peak values are tested: the dependent\r\nvariable becomes, for each fold, the variable provided by 'dv' at that specific peak.\r\nAnother LMEM is then computed by using this newly created variable. One problem with\r\nthis approach is that peak values can be all over the places, depending on your data.\r\nAlso, choosing the time-points based on the maximum value in the training dataset can\r\noccasionally decrease the precision of the estimate or give overfitting. You may use\r\nthis approach if you are confident that a specific effect only has an effect at a specific\r\nwindow; effects with multiple windows - e.g., an early and late impact on pupil size - may\r\nnot be properly captured with this approach. Therefore,\r\nin addition to this procedure, a very coarse consensus is seek by assessing,\r\nacross all folds and effects, which time points resulted in t-values above a certain\r\nthreshold; if the same time points pop out consistently across folds (e.g.,= 'consensus_thresh' % of the times), then the time point is retained; all time-pointsretained in the consensus are collapsed (averaged), and a final LMEM is performed withthese time points. This can be interpreted more similarly to a cluster-basedpermutation test (although it is not the same). — decode_signal","text":"data data.frame containing necessary variables. formula 'lme4'-style formula, passed string. dv string indicating name dependent variable. time string indicating name time variable. id string indicating name id (participant) variable. trial string indicating name trial variable. nfolds Number folds split trials . Defaults 4. t_thresh Used seek consensus: minimum t-value required push time-point forward. consensus_thresh minimum proportion time-points must 't_thresh' across folds order keep time-point consensus.","code":""},{"path":"/reference/decode_signal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Highlights and test the time-course of effects through crossvalidation\r\nThis function is meant to implement, roughly and with no assurance of full compatibility whatsoever,\r\nthe procedure proposed by Mathôt and Vilotijević (2022, Behavior Research Methods).\r\nFirst, each trial (i.e., one complete time series) is assigned to one fold in a\r\ndeterministic fashion (the first trial to the first fold, then the second trial to\r\nthe second fold, etc.). In doing that, there is no regard of how conditions are\r\ndistributed across folds, i.e. data may be slightly unbalanced; thus, you should think\r\ncarefully as to whether this strategy applies to your design (e.g., blocked conditions).\r\nThen, data are separated for each time-point, and a LMEM as specified by the 'formula'\r\nparameter, which is passed to 'lmerTest::lmer', is performed by iteratively leaving\r\none fold out. This results in a table, with as many rows as effects implied\r\nby the formula by 'nfolds', summarising which time point had a peak t-value (in absolute value)\r\nin the trained folds. In a separate table these peak values are tested: the dependent\r\nvariable becomes, for each fold, the variable provided by 'dv' at that specific peak.\r\nAnother LMEM is then computed by using this newly created variable. One problem with\r\nthis approach is that peak values can be all over the places, depending on your data.\r\nAlso, choosing the time-points based on the maximum value in the training dataset can\r\noccasionally decrease the precision of the estimate or give overfitting. You may use\r\nthis approach if you are confident that a specific effect only has an effect at a specific\r\nwindow; effects with multiple windows - e.g., an early and late impact on pupil size - may\r\nnot be properly captured with this approach. Therefore,\r\nin addition to this procedure, a very coarse consensus is seek by assessing,\r\nacross all folds and effects, which time points resulted in t-values above a certain\r\nthreshold; if the same time points pop out consistently across folds (e.g.,= 'consensus_thresh' % of the times), then the time point is retained; all time-pointsretained in the consensus are collapsed (averaged), and a final LMEM is performed withthese time points. This can be interpreted more similarly to a cluster-basedpermutation test (although it is not the same). — decode_signal","text":"list including: peaks retained (left-) fold; test retained, cross-validated peaks; test consensus time-points, ; list time-points retained consensus effect.","code":""},{"path":"/reference/decode_signal_g.html","id":null,"dir":"Reference","previous_headings":"","what":"Highlights and test the time-course of effects through crossvalidation — decode_signal_g","title":"Highlights and test the time-course of effects through crossvalidation — decode_signal_g","text":"'decode_signal()' except powered 'glmer()' thus performs generalized LMEMs.","code":""},{"path":"/reference/decode_signal_g.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Highlights and test the time-course of effects through crossvalidation — decode_signal_g","text":"","code":"decode_signal_g(   data,   formula,   dv,   time,   id,   trial,   nfolds = 4,   t_thresh = 2,   consensus_thresh = 0.75,   ... )"},{"path":"/reference/decode_signal_g.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Highlights and test the time-course of effects through crossvalidation — decode_signal_g","text":"data data.frame containing necessary variables. formula 'lme4'-style formula, passed string. dv string indicating name dependent variable. time string indicating name time variable. id string indicating name id (participant) variable. trial string indicating name trial variable. nfolds Number folds split trials . Defaults 4. t_thresh Used seek consensus: minimum t-value required push time-point forward. consensus_thresh minimum proportion time-points must 't_thresh' across folds order keep time-point consensus. ... params 'glmer()', e.g. \"family\".","code":""},{"path":"/reference/decode_signal_g.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Highlights and test the time-course of effects through crossvalidation — decode_signal_g","text":"list including: peaks retained (left-) fold; test retained, cross-validated peaks; test consensus time-points, ; list time-points retained consensus effect.","code":""},{"path":"/reference/detect_change.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect a change in a column, and returns an incremental counter — detect_change","title":"Detect a change in a column, and returns an incremental counter — detect_change","text":"Mostly used, e.g., case ET file provide Trial number column. function monitors occurrences 'key' value , value appears first time, increases counter 1. example, return counter first occurrences \"target\" \"Event\" column, thus returning putative trial number assuming target repeated iteration. default, column track filled downward. Also, empty lines changed NA. final remark, may need clean lines assigned trial (sometimes, e.g., eyetracker needs time warm ).","code":""},{"path":"/reference/detect_change.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect a change in a column, and returns an incremental counter — detect_change","text":"","code":"detect_change(vector, key)"},{"path":"/reference/detect_change.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect a change in a column, and returns an incremental counter — detect_change","text":"vector vector variable tracked. key Value track, first repetition update counter.","code":""},{"path":"/reference/detect_change.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect a change in a column, and returns an incremental counter — detect_change","text":"numeric vector returning counter - function can thus used tidyverse-style pipelines grouping (e.g., ID).","code":""},{"path":"/reference/downsample_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Recode a Time variable to a different granularity — downsample_time","title":"Recode a Time variable to a different granularity — downsample_time","text":"time variable passed function corresponding downsampled time bin returned. One 'to_ms' 'to_hz' must null","code":""},{"path":"/reference/downsample_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recode a Time variable to a different granularity — downsample_time","text":"","code":"downsample_time(Time, to_ms = 25, to_hz = NULL)"},{"path":"/reference/downsample_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recode a Time variable to a different granularity — downsample_time","text":"Time vector variable indicating elapsed time, ms, aligned origin - .e., timestamp. to_ms many ms bins must . to_hz many hertz new sampling rate .","code":""},{"path":"/reference/downsample_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recode a Time variable to a different granularity — downsample_time","text":"numeric time vector recoded time bins.","code":""},{"path":"/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Linearly interpolate signal provided quality checks are met, else only returns NAs — interpolate","title":"Linearly interpolate signal provided quality checks are met, else only returns NAs — interpolate","text":"used linearly interpolate data provided successful quality controls. controls met, returned vector vector equal length composed NAs. Quality checks general (.e., overall percentage available non NA data) relative consecutive gaps signal, must exceed given threshold. Thresholds refer maximum rate (percentage) entries allowed NAs; results interpolated thresholds.","code":""},{"path":"/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linearly interpolate signal provided quality checks are met, else only returns NAs — interpolate","text":"","code":"interpolate(   vector,   extend_blink = pp_options(\"extend_blink\"),   overall_thresh = pp_options(\"overall_thresh\"),   consecutive_thresh = pp_options(\"consecutive_thresh\") )"},{"path":"/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linearly interpolate signal provided quality checks are met, else only returns NAs — interpolate","text":"vector vector interpolate. extend_blink NAs extended many samples prior interpolation. gets rid signal may compromised close proximity blink. overall_thresh Overall quality threshold: e.g., total amount data allowed missing original vector. consecutive_thresh Consecutive gaps signal: e.g., total amount data allowed missing original vector consecutively.","code":""},{"path":"/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linearly interpolate signal provided quality checks are met, else only returns NAs — interpolate","text":"numeric vector interpolated data NAs quality checks met.","code":""},{"path":"/reference/plot_loadings.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot loadings of a reduced time-series — plot_loadings","title":"Plot loadings of a reduced time-series — plot_loadings","text":"function active development. meant depict loadings components obtained reducing pupillary time-series, ease interpretation.","code":""},{"path":"/reference/plot_loadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot loadings of a reduced time-series — plot_loadings","text":"","code":"plot_loadings(name, data)"},{"path":"/reference/plot_loadings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot loadings of a reduced time-series — plot_loadings","text":"name string indicating component depict (e.g., \"PC1\"). data object returned , e.g., 'reduce_PCA'.","code":""},{"path":"/reference/plot_loadings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot loadings of a reduced time-series — plot_loadings","text":"plot powered 'ggplot2'.","code":""},{"path":"/reference/pp_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set or get options for Pupilla's preprocessing parameters — pp_options","title":"Set or get options for Pupilla's preprocessing parameters — pp_options","text":"Set get options Pupilla's preprocessing parameters","code":""},{"path":"/reference/pp_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set or get options for Pupilla's preprocessing parameters — pp_options","text":"","code":"pp_options(...)"},{"path":"/reference/pp_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set or get options for Pupilla's preprocessing parameters — pp_options","text":"thresh speed_clean Threshold (z point absolute value) values marked NA. speed_method speed_clean Whether 'thresh' z-score ('z'), deviant values omitted , values threshold ('z-dynamic'). 'abs' used instead precise absolute value speed supplied. extend_by speed_clean Number samples starting deviant speed values stripped (e.g., signal proximity blinks may biased well). island_size speed_clean Islands signal midst NAs removed smaller equal threshold (amount samples). extend_blink interpolate NAs extended many samples prior interpolation. gets rid signal may compromised close proximity blink. overall_thresh interpolate Overall quality threshold: e.g., total amount data allowed missing original vector. consecutive_thresh interpolate Consecutive gaps signal: e.g., total amount data allowed missing original vector consecutively. spar smooth Smoothing factor 'smooth.spline()'.","code":""},{"path":"/reference/pre_process.html","id":null,"dir":"Reference","previous_headings":"","what":"A convenience function to preprocess pupillometry data — pre_process","title":"A convenience function to preprocess pupillometry data — pre_process","text":"function calls, order, 'speed_clean', 'interpolate', 'smooth_vector' packages. Parameters can changed package options, .e. 'Pupilla::pp_options()'. Warning, best preprocessing parameters may deviate defaults used , mindful!","code":""},{"path":"/reference/pre_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A convenience function to preprocess pupillometry data — pre_process","text":"","code":"pre_process(vector, time)"},{"path":"/reference/pre_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A convenience function to preprocess pupillometry data — pre_process","text":"vector vector variable cleaned time vector variable indicating elapsed time, needed compute velocity.","code":""},{"path":"/reference/pre_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A convenience function to preprocess pupillometry data — pre_process","text":"numeric vector processed requested per default. .","code":""},{"path":"/reference/read_TOBII.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads and imports TOBII eye-tracking and behavioral data — read_TOBII","title":"Reads and imports TOBII eye-tracking and behavioral data — read_TOBII","text":"two type data usually separate sometimes must merged create one single file eyetracking experimental details (e.g., conditions), unless specified via code experiment builder. function written OpenSesame-like csv files mind, though compatibility programs (e.g., e-prime) may achieved provided files converted csv format. optimized Windows machines - may encounter adress errors Macs.","code":""},{"path":"/reference/read_TOBII.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads and imports TOBII eye-tracking and behavioral data — read_TOBII","text":"","code":"read_TOBII(   ID,   path = getwd(),   start_filename = \"subject-\",   append_TOBII = \"_TOBII_output.tsv\",   skip = 7,   separate_behavioral = TRUE )"},{"path":"/reference/read_TOBII.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads and imports TOBII eye-tracking and behavioral data — read_TOBII","text":"ID integer corresponding one participant's ID. attached current path order locate two files read. Also, One variable named p_ID attached ET BD files. vector supplied, files read lapply merged. path Defaults getwd() can specified otherwise. Files searched starting location. start_filename string, defaults \"subject-\". Usually files start string, regardless nature. Usually names built concatenating path, start_filename, ID, append_TOBII (eye-tracking data, else \".csv\"). append_TOBII string, defaults \"_TOBII_output.tsv\" indicates text tells eye-tracking behavioral files apart. Usually names built concatenating path, start_filename, ID, append_TOBII (eye-tracking data, else \".csv\"). skip Integer. amount lines skip eye-tracking file, .e. many lines header encountered. passed data.table::fread(). separate_behavioral defaults TRUE. FALSE, reads eye-tracking data","code":""},{"path":"/reference/read_TOBII.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads and imports TOBII eye-tracking and behavioral data — read_TOBII","text":"list one two DFs, one eye-tracking data, one behavioral data (requested).","code":""},{"path":"/reference/reduce_ICA.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce time-series to few Independent Components — reduce_ICA","title":"Reduce time-series to few Independent Components — reduce_ICA","text":"function active development. meant reduce entire time-series normalized baseline-corrected pupillary data just scores obtained Independent Component Analysis. ICA effective way reduce data dimensionality manageable dependent variables, may additionally help precise estimates (fingerprints) pupil signal underlying cognitive processes. functions use 'ica::icafast'.","code":""},{"path":"/reference/reduce_ICA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce time-series to few Independent Components — reduce_ICA","text":"","code":"reduce_ICA(data, dv, time, id, trial, Ncomp = NULL, scaling = FALSE, add)"},{"path":"/reference/reduce_ICA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce time-series to few Independent Components — reduce_ICA","text":"data data.frame containing necessary variables. dv string indicating name dependent variable. time string indicating name time variable. id string indicating name id (participant) variable. trial string indicating name trial variable. Ncomp Number components retain. default (NULL) automatically retains 95% explained variance. Ncomp== \"\" returns components. Ncomp <1 interpreted user wishes retain given proportion variance (e.g. 0.6). scaling Whether variables, .e. pupil size timepoint, scaled beforehand. Defaults FALSE assuming measures already normalized (z-scores) baseline-corrected. add String(s) indicating variables names, , appendend scores dataframe.","code":""},{"path":"/reference/reduce_ICA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce time-series to few Independent Components — reduce_ICA","text":"list including processed data, scores loadings dataframes, ICA object useful prediction new data.","code":""},{"path":"/reference/reduce_PCA.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce time-series to few Principal Components — reduce_PCA","title":"Reduce time-series to few Principal Components — reduce_PCA","text":"function active development. meant reduce entire time-series normalized baseline-corrected pupillary data just scores obtained Principal Component Analysis. PCA effective way reduce data dimensionality manageable dependent variables, may additionally help precise estimates (fingerprints) pupil signal underlying cognitive processes.","code":""},{"path":"/reference/reduce_PCA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce time-series to few Principal Components — reduce_PCA","text":"","code":"reduce_PCA(data, dv, time, id, trial, Ncomp = NULL, scaling = FALSE, add)"},{"path":"/reference/reduce_PCA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce time-series to few Principal Components — reduce_PCA","text":"data data.frame containing necessary variables. dv string indicating name dependent variable. time string indicating name time variable. id string indicating name id (participant) variable. trial string indicating name trial variable. Ncomp Number components retain. default (NULL) automatically retains 95% explained variance. Ncomp== \"\" returns components. Ncomp <1 interpreted user wishes retain given proportion variance (e.g. 0.6). scaling Whether variables, .e. pupil size timepoint, scaled beforehand. Defaults FALSE assuming measures already normalized (z-scores) baseline-corrected. add String(s) indicating variables names, , appendend scores dataframe.","code":""},{"path":"/reference/reduce_PCA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce time-series to few Principal Components — reduce_PCA","text":"list including processed data, scores loadings dataframes, PCA object useful prediction new data.","code":""},{"path":"/reference/smooth_vector.html","id":null,"dir":"Reference","previous_headings":"","what":"Smooth a time series through cubic splines — smooth_vector","title":"Smooth a time series through cubic splines — smooth_vector","text":"can used simple smoothing function, case eyetracking data can used low-pass filter, useful correct artifacts, blinks, etc.","code":""},{"path":"/reference/smooth_vector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smooth a time series through cubic splines — smooth_vector","text":"","code":"smooth_vector(vector, time, spar = pp_options(\"spar\"))"},{"path":"/reference/smooth_vector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smooth a time series through cubic splines — smooth_vector","text":"vector vector variable smoothed. time vector variable indicating elapsed time. spar Smoothing factor 'smooth.spline()'.","code":""},{"path":"/reference/smooth_vector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smooth a time series through cubic splines — smooth_vector","text":"numeric vector smoothed requested.","code":""},{"path":"/reference/speed_clean.html","id":null,"dir":"Reference","previous_headings":"","what":"Help identifying artifacts with a speed-based criterion — speed_clean","title":"Help identifying artifacts with a speed-based criterion — speed_clean","text":"signal vector stripped values exceeding given threshold, computed basis absolute speed signal increase decrease.","code":""},{"path":"/reference/speed_clean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Help identifying artifacts with a speed-based criterion — speed_clean","text":"","code":"speed_clean(   vector,   time,   thresh = pp_options(\"thresh\"),   speed_method = pp_options(\"speed_method\"),   extend_by = pp_options(\"extend_by\"),   island_size = pp_options(\"island_size\") )"},{"path":"/reference/speed_clean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Help identifying artifacts with a speed-based criterion — speed_clean","text":"vector vector variable cleaned time vector variable indicating elapsed time, needed compute velocity. thresh Threshold (z point absolute value) values marked NA. speed_method Whether 'thresh' z-score ('z'), deviant values omitted , values threshold ('z-dynamic'). 'abs' used instead precise absolute value speed supplied. extend_by Number samples starting deviant speed values stripped (e.g., signal proximity blinks may biased well). island_size Islands signal midst NAs removed smaller equal threshold (amount samples).","code":""},{"path":"/reference/speed_clean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Help identifying artifacts with a speed-based criterion — speed_clean","text":"numeric vector cleaned requested.","code":""}]
