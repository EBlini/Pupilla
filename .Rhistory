#
# eDFs$paL[eDFs$Blink==1]= 0
# plot(y = eDFs$paL[1:8000], x=eDFs$time_rel[1:8000])
# points(x= eDFs$time_rel[1:8000][eDFs$Blink==1],
#        y = eDFs$paL[1:8000][eDFs$Blink==1], col= "red")
#eDF$events$message
# eDF$events$message= ifelse(eDF$events$message== "", NA,
#                            eDF$events$message)
#
# eDF$events= tidyr::fill(eDF$events,
#                         message, .direction = "down")
eDF$events= eDF$events[eDF$events$type== "MESSAGEEVENT",]
#keep events here
keep_events= c("fixation",
"cue", "target1", "sign", "target2", "calculation",
"response",
"post_resp_fix", "Wait_Feedback", "feedback")
eDF$events= eDF$events[eDF$events$message %in% keep_events,]
eDFs$Event= NA
for (i in 1:nrow(eDF$events)){
trial= eDF$events[i, "trial"]
from= eDF$events[i, "sttime"]
msg= eDF$events[i, "message"]
eDFs$Event[eDFs$trial== trial & eDFs$time>= from]= msg
}
eDFs= eDFs[!is.na(eDFs$Event),]
#table(eDFs$Event)
plot(y = eDFs$paL[eDFs$trial== 55],
x=eDFs$time_rel[eDFs$trial== 55])
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "fixation"])))
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "cue"])))
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "target1"])))
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "calculation"])))
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "feedback"])))
# sum(diff(na.omit(eDFs$time_rel[eDFs$Event== "Wait_Feedback"])))
# #actually is ok... it is the last message...
# with(eDFs[eDFs$Event== "feedback",],
#      tapply(time_rel, trial, function(x)sum(diff(x), na.rm= T)))
# with(eDFs[eDFs$Event== "Wait_Feedback",],
#      tapply(time_rel, trial, function(x)sum(diff(x), na.rm= T)))
#table(eDFs$Event)
#
# table(eDFs$trial)
# table(DF$Trial)
ET= eDFs
ET$Subject= subj
#whether it's practice or experiment
ET$Phase= copy_variable("Phase", id_var = "Subject",
constrained_var = "trial",
larger_df = ET, smaller_df = DF)
ET$Result= copy_variable("Result", id_var = "Subject",
constrained_var = "trial",
larger_df = ET, smaller_df = DF)
ET$Response= copy_variable("response", id_var = "Subject",
constrained_var = "trial",
larger_df = ET, smaller_df = DF)
ET$Accuracy= ifelse(ET$Response== ET$Result,
"correct",
"incorrect")
ET$Cue= copy_variable("Cue", id_var = "Subject",
constrained_var = "trial",
larger_df = ET, smaller_df = DF)
#table(ET$Phase, ET$trial)
#discard practice
#discard practice
ET= ET[ET$Phase== "experiment",]
BD= DF[DF$Phase== "experiment",]
ET$Pupil= ET$paL
sum(is.na(ET$Event))
ET= ET[!is.na(ET$Event),]
#very imp!!!
ET$Pupil= ifelse(ET$Pupil==0, NA, ET$Pupil)
hist(ET$Pupil)
#clean a little bit, omit values too small
cut_off= mean(ET$Pupil, na.rm= T) - 2.5*sd(ET$Pupil, na.rm= T)
ET$Pupil= ifelse(ET$Pupil < cut_off, NA, ET$Pupil)
# speed= ET %>%
#   group_by(Subject, trial) %>%
#   mutate(speed= c(NA, diff(Pupil)/diff(time_rel))) %>%
#   select(speed)
#
# plot(density(speed$speed, na.rm=T))
# quantile(speed$speed, 0.95, na.rm=T)
# #calculate speed
# #calculate speed here
#
# vector= ET$Pupil[ET$Subject== 1 & ET$trial== 5]
# time= ET$time_rel[ET$Subject== 1 & ET$trial== 5]
# plot(x= time, y= vector)
#pp_options()
pp_options("thresh"= 3)
pp_options("speed_method"= "z")
#pp_options("extend_by"= 5)
pp_options("extend_by"= 25)
pp_options("extend_blink"= 25)
pp_options("spar"= 0.8)
#pp_options("spar"= 0.005)
vector= ET$Pupil[ET$trial== 33]
time= ET$time_rel[ET$trial== 33]
plot(x= time, y= vector)
vector= ET$Pupil[ET$trial== 33]
time= ET$time_rel[ET$trial== 33]
plot(x= time, y= vector)
spar= 0.05
time2= time[!is.na(vector)]
vector2= vector[!is.na(vector)]
ss= smooth.spline(time2, vector2, #spar= spar)
spar= NULL, cv= FALSE)
lp= predict(ss, time)$y
points(x= time, y= lp, col= "red")
ss$spar
vector= ET$Pupil[ET$trial== 33]
time= ET$time_rel[ET$trial== 33]
plot(x= time, y= vector)
spar= 0.05
time2= time[!is.na(vector)]
vector2= vector[!is.na(vector)]
ss1= smooth.spline(time2, vector2, spar= 0.1)
ss2= smooth.spline(time2, vector2, spar= 0.8)
ss= smooth.spline(time2, vector2, #spar= spar)
spar= NULL, cv= FALSE)
lp1= predict(ss1, time)$y
lp2= predict(ss2, time)$y
lp= predict(ss, time)$y
lp1= predict(ss1, time)$y
lp2= predict(ss2, time)$y
lp= predict(ss, time)$y
points(x= time, y= ss1$y, col= "red")
points(x= time, y= lp, col= "red")
points(x= time, y= lp1, col= "blue")
points(x= time, y= lp2, col= "green")
ss$spar
points(x= time, y= lp, col= "red")
vector= ET$Pupil[ET$trial== 8]
time= ET$time_rel[ET$trial== 8]
plot(x= time, y= vector)
time2= time[!is.na(vector)]
vector2= vector[!is.na(vector)]
ss1= smooth.spline(time2, vector2, spar= 0.1)
ss2= smooth.spline(time2, vector2, spar= 0.8)
ss= smooth.spline(time2, vector2, #spar= spar)
spar= NULL, cv= FALSE)
lp1= predict(ss1, time)$y
lp2= predict(ss2, time)$y
lp= predict(ss, time)$y
points(x= time, y= lp1, col= "blue")
points(x= time, y= lp2, col= "green")
points(x= time, y= lp, col= "red")
ss$spar
vector= ET$Pupil[ET$trial== 9]
time= ET$time_rel[ET$trial== 9]
plot(x= time, y= vector)
time2= time[!is.na(vector)]
vector2= vector[!is.na(vector)]
ss1= smooth.spline(time2, vector2, spar= 0.1)
ss2= smooth.spline(time2, vector2, spar= 0.8)
ss= smooth.spline(time2, vector2, #spar= spar)
spar= NULL, cv= FALSE)
lp1= predict(ss1, time)$y
lp2= predict(ss2, time)$y
lp= predict(ss, time)$y
points(x= time, y= lp1, col= "blue")
points(x= time, y= lp2, col= "green")
points(x= time, y= lp, col= "red")
setwd("I:\\Il mio Drive\\Esperimenti\\01 math anxiety\\task\\data")
library("readxl")
library("dplyr")
library("ggplot2")
library("Pupilla")
files= list.files(path = paste0(getwd(), "\\pp_data"),
pattern = ".RDS")
ET= {}
for (i in files){
temp= readRDS(paste0(getwd(), "\\pp_data\\", i))
ET= rbind(ET, temp)
}
table(ET$Subject)
#pcs to divide in low/high anxiety
PCs= readRDS("Quest_pp.RData")
ET=
ET %>%
#riallinea tempo
#separatamente per prima e seconda fase???
mutate(Part= ifelse(Event %in% c("feedback",
"post_resp_fix",
"Wait_Feedback"), 2, 1)) %>%
group_by(Subject, Part, trial) %>%
mutate(Time= ifelse(Part== 1,
Time - Time[Event== "cue"][1],
Time - Time[Event== "Wait_Feedback"][1] + 20000))
#realign second part, set to 20000
ET= ET %>%
#sottrai baseline
group_by(Subject, trial) %>%
mutate(Pupil= Pupil - median(Pupil[Event== "fixation"]))
#now polish
legit_time= c(seq(25, 18000, 25), seq(20025, 28000, 25))
ET= ET %>% filter(Time %in% legit_time)
# #resort!
ET= ET %>%
arrange(Subject, trial, Time) %>%
group_by(Subject, trial, Time, Cue, Accuracy) %>%
summarise(Pupil= mean(Pupil))
# #resort!
ET= ET %>%
arrange(Subject, trial, Time) %>%
group_by(Subject, trial, Time, Cue, Accuracy) %>%
summarise(Pupil= mean(Pupil))
unique(ET$Time)
data= ET
#data= ET[ET$Subject<55,]
dv= "Pupil"
time= "Time"
id= "Subject"
trial= "trial"
add= c("Cue")
Ncomp= NULL
res= reduce_PCA(data,
dv,
time,
id,
trial,
Ncomp= NULL,
scaling= F,
add)
res= reduce_PCA(data,
dv,
time,
id,
trial,
Ncomp= NULL,
scaling= F,
add)
#' Reduce time-series to few Principal Components
#'
#' This function is under active development. It is meant to
#' reduce the entire time-series of normalized and baseline-corrected
#' pupillary data in just a few scores obtained by Principal
#' Component Analysis. PCA is an effective way to reduce data dimensionality
#' as to have more manageable dependent variables, which may additionally
#' help having more precise estimates (or fingerprints) of pupil signal
#' and the underlying cognitive processes.
#'
#' @param data A data.frame containing all the necessary variables.
#' @param dv A string indicating the name of the dependent variable.
#' @param time A string indicating the name of the time variable.
#' @param id A string indicating the name of the id (participant) variable.
#' @param trial A string indicating the name of the trial variable.
#' @param Ncomp Number of components to retain. The default (NULL) automatically
#' retains 95% of the explained variance. If Ncomp== "all" returns all
#' the components. If Ncomp <1 this is interpreted as if the user wishes to retain
#' a given proportion of variance (e.g. 0.6).
#' @param scaling Whether variables, i.e. pupil size for each timepoint,
#' should be scaled beforehand. Defaults to FALSE assuming that measures
#' are already normalized (with z-scores) and baseline-corrected.
#' @param add String(s) indicating which variables names, if any, should
#' be appendend to the scores dataframe.
#' @return A list including the processed data, the scores and loadings dataframes,
#' and the PCA object useful for prediction of new data.
#'
#' @export
reduce_PCA= function(data,
dv,
time,
id,
trial,
Ncomp= NULL,
scaling= FALSE,
add){
#first change names for your convenience; it's easier this way
DF= data.frame(data)
DF$dv= DF[,colnames(DF)== dv]
DF$time= DF[,colnames(DF)== time]
DF$subject= DF[,colnames(DF)== id]
DF$trial= DF[,colnames(DF)== trial]
#these will be the rows
DF$interaction= interaction(DF$trial, DF$subject)
#set unique levels
order= unique(DF$time)
# empty= rep(NA, length(order))
# names(empty)= order
#
#get pupil as a vector
rsmat= lapply(levels(DF$interaction), function(x){
#extract time series
vec_pupil= DF$dv[DF$interaction== x]
if(length(vec_pupil)!= length(order))(vec_pupil= rep(NA, length(order)))
return(vec_pupil)
})
#merge in columns
rsmat2= do.call(cbind, rsmat)
#check NAs here and warn
if(sum(is.na(rsmat2))>0){
warning("NAs is the data will be discarded:
check the data (unequal timepoints maybe?)!")
}
#discard
ind= apply(rsmat2, 2, function(x)sum(is.na(x))==0)
orig= levels(DF$interaction)[ind]
rsmat3= rsmat2[,ind]
rs_mat= t(rsmat3)
#return scaling info just in case
col_means= apply(rs_mat, 2, mean)
col_sd= apply(rs_mat, 2, sd)
#run PCA
PCA= prcomp(rs_mat, scale= scaling)
#summary
summaryPCA= summary(PCA)$importance
#how many comps for 95% variance
Ncomp95= as.numeric(which(summaryPCA[3,]>0.95)[1])
#Ncomp changes accordingly
if(is.null(Ncomp)){
Ncomp= Ncomp95
}
if(Ncomp== "all"){
Ncomp= length(summaryPCA[3,])
}
if (Ncomp<1){
Ncomp= as.numeric(which(summaryPCA[3,]>Ncomp)[1])
}
if (Ncomp>length(summaryPCA[3,])){
warning("You asked for too many components!")
}
Loadings= PCA$rotation[,1:Ncomp]
rownames(Loadings)= order
Scores= predict(PCA,
newdata= rs_mat)[,1:Ncomp]
#now append relevant information
Scores= data.frame(Scores)
Scores$id= NA
Scores$trial= NA
for(i in 1:nrow(Scores)){
sel= orig[i]
Scores$id[i]= DF$subject[DF$interaction== sel][1]
Scores$trial[i]= DF$trial[DF$interaction== sel][1]
}
if(!is.null(add)){
for (a in add){
Scores$add= NA
for(i in 1:nrow(Scores)){
sel= orig[i]
Scores$add[i]= DF[DF$interaction== sel, a][1]
}
colnames(Scores)[colnames(Scores)=="add"]= a
}
}
res= list(rs_mat= rs_mat,
summaryPCA= summaryPCA,
Loadings= Loadings,
Scores= Scores,
PCA= PCA,
scaling= list(M= col_means, SD= col_sd))
return(res)
}
res= reduce_PCA(data,
dv,
time,
id,
trial,
Ncomp= NULL,
scaling= F,
add)
Weights= res$Scores
Pupil_PCs= Weights %>%
group_by(id, Cue) %>%
summarise(across(c(PC1, PC2, PC3, PC4, PC5, PC6, PC7),
mean))
Pupil_PCs
Pupil_PCs_diff=cbind(Pupil_PCs_diff,
AMAS= c(scale(PCs$AMAS)),
ACC= c(scale(PCs$ACC)),
Latency= c(scale(PCs$Latency)))
Pupil_PCs_diff= Pupil_PCs %>%
group_by(id) %>%
summarise(across(c(PC1, PC2, PC3, PC4, PC5, PC6, PC7),
function(x)(x[Cue== "hard"]-x[Cue== "easy"])))
Pupil_PCs_diff=cbind(Pupil_PCs_diff,
AMAS= c(scale(PCs$AMAS)),
ACC= c(scale(PCs$ACC)),
Latency= c(scale(PCs$Latency)))
Pupil_PCs= Pupil_PCs %>%
group_by(id) %>%
summarise(across(c(PC1, PC2, PC3, PC4, PC5, PC6, PC7),
mean))
#plot loadings
my_plot= function(name, data= res){
#set data
dv= data$Loadings[, name]
time= as.numeric(rownames(data$Loadings))
DF= data.frame(Time= time, dv= dv)
DF$Part= ifelse(DF$Time < 20000, 1, 2)
DF$Time[DF$Part==2]= DF$Time[DF$Part==2] - 20000
DF$Part= ifelse(DF$Part==2, "Second Part", "First Part")
percentile= ecdf(abs(DF$dv))
DF$Color= percentile(abs(DF$dv) + min(abs(DF$dv)))
limit= c(min(dv), max(dv))
if(limit[2]<0)(limit[2]= 0)
if(limit[1]>0)(limit[1]= 0)
#theme
commonTheme= list(#theme_bw(),
theme(text= element_text(size= 16,
face="bold")),
xlab("Time (ms)"), ylab("Loading"),
ylim(limit),
ggtitle(name,
subtitle= paste0("Explained variance: ",
round(data$summaryPCA["Proportion of Variance", name], 2)))
)
p=
ggplot(DF, aes(x= Time, y= dv)) +
geom_point(aes(color= Color), size= 1.3, show.legend = F) +
facet_wrap("Part", ncol = 2, scales = "free_x") +
scale_color_gradient(low = "yellow", high = "red") +
geom_hline(yintercept = 0,
color= "black", linetype= "dashed", size= 1.2) +
commonTheme
return(p)
}
pc1= my_plot("PC1")
pc1
data= res
dv= data$Loadings[, name]
percentile= ecdf(abs(DF$dv))
name= "PC1"
dv= data$Loadings[, name]
time= as.numeric(rownames(data$Loadings))
DF= data.frame(Time= time, dv= dv)
percentile= ecdf(abs(DF$dv))
DF$Color= percentile(abs(DF$dv) + min(abs(DF$dv)))
DF$Color
limit= c(min(dv), max(dv))
if(limit[2]<0)(limit[2]= 0)
if(limit[1]>0)(limit[1]= 0)
ggplot2::ylim
#theme
commonTheme= list(#theme_bw(),
ggplot2::theme(text= ggplot2::element_text(size= 16,
face="bold")),
ggplot2::xlab("Time (ms)"),
ggplot2::ylab("Loading"),
ggplot2::ylim(limit),
ggplot2::ylimggtitle(name,
subtitle= paste0("Explained variance: ",
round(data$summaryPCA["Proportion of Variance", name], 2)))
)
#theme
commonTheme= list(#theme_bw(),
ggplot2::theme(text= ggplot2::element_text(size= 16,
face="bold")),
ggplot2::xlab("Time (ms)"),
ggplot2::ylab("Loading"),
ggplot2::ylim(limit),
ggplot2::ggtitle(name,
subtitle= paste0("Explained variance: ",
round(data$summaryPCA["Proportion of Variance", name], 2)))
)
#' Plot loadings of a reduced time-series
#'
#' This function is under active development. It is
#' meant to depict the loadings of components obtained by reducing
#' pupillary time-series, as to ease interpretation.
#'
#' @param name A string indicating the component to depict (e.g., "PC1").
#' @param data An object as returned by, e.g., 'reduce_PCA'.
#' @return A plot powered by 'ggplot2'.
#'
#' @export
plot_loadings= function(name,
data){
#set data
dv= data$Loadings[, name]
time= as.numeric(rownames(data$Loadings))
DF= data.frame(Time= time, dv= dv)
#pretty color scheme
percentile= ecdf(abs(DF$dv))
DF$Color= percentile(abs(DF$dv) + min(abs(DF$dv)))
limit= c(min(dv), max(dv))
if(limit[2]<0)(limit[2]= 0)
if(limit[1]>0)(limit[1]= 0)
#theme
commonTheme= list(#theme_bw(),
ggplot2::theme(text= ggplot2::element_text(size= 16,
face="bold")),
ggplot2::xlab("Time (ms)"),
ggplot2::ylab("Loading"),
ggplot2::ylim(limit),
ggplot2::ggtitle(name,
subtitle= paste0("Explained variance: ",
round(data$summaryPCA["Proportion of Variance", name], 2)))
)
p=
ggplot2::ggplot(DF, ggplot2::aes(x= Time, y= dv)) +
ggplot2::geom_point(ggplot2::aes(color= Color),
size= 1.3, show.legend = F) +
ggplot2::scale_color_gradient(low = "yellow", high = "red") +
ggplot2::geom_hline(yintercept = 0,
color= "black", linetype= "dashed",
linewidth= 1.2) +
commonTheme
return(p)
}
plot_loadings("PC1", res)
plot_loadings("PC2", res)
library(Pupilla)
install.packages("settings")
pkgdown::build_site()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_articles()
pkgdown::build_reference()
pkgdown::build_articles()
devtools::load_all(".")
