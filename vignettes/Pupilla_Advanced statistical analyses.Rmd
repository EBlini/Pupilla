---
title: "Pupilla::Advanced statistical analyses"
author: "Elvio Blini"
date: "February 2024"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Pupilla::Advanced statistical analyses}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
resource_files:
    - data/zhou et al 2022.RData

---

```{r setup, include=FALSE}
#knitr::knit_engines$set(python= reticulate::eng_python)
options(width = 999)
Sys.setenv(LANG = "en")
knitr::opts_chunk$set(
  echo= T,
  #fig.path = "images/",
  eval= T,
  fig.width = 7, 
  fig.height = 7
)

```

## Introduction

`Pupilla` gathers several functions that are designed to facilitate the analysis of pupillometry experiments as commonly performed in cognitive neuroscience, e.g. event-related designs, although its use could be much more general.
In this vignette we cover the statistical modelling step. `Pupilla` provides functions to implement two approaches:

  1. Crossvalidated LMEMs as in [Mathôt & Vilotijević, 2022](https://link.springer.com/article/10.3758/s13428-022-01957-7). 
    
  2.  An original - and thus not yet fully validated in scientific articles - approach through feature reduction.
  
We will use the data from [Zhou et al., 2022](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13194) for a better comparison - there are indeed differences in the implementation of LMEMs in R and Python, on top of course of the specific choices that I've made within this package.

I am grateful to the authors for sharing their data and allowing us to do reproducible, exploitable science.

## Prepping the data

We will need the following R packages:

```{r}
library("dplyr")
library("Pupilla")
library("ggplot2")
library("emmeans")
library("lmerTest")
library("zoo")

options(dplyr.summarise.inform = FALSE)

```

For the python interface:

```{r eval= F}
library("reticulate")

# #create python environment
#conda_create("my_env")
#use_miniconda("my_env")

# #install packages in the environment
# py_install("pandas")
# py_install("datamatrix")

```

Note that package `reticulate` requires a python environment and the installation of the required modules. This package is only needed to read `.pkl` files directly from within R, that is the original format of the data.  
Accordingly, the following chunk is run with python as engine - that is, you need to include it within a reticulate repl if not using markdown. We use it to read the [original data](https://github.com/smathot/time_series_test), convert it to a `pandas` dataframe, and then move it the R environment.

```{python, eval= F}
from datamatrix import io, convert

address= "data\\zhou_et_al_2021.pkl"

data= io.readpickle(address)

data= convert.to_pandas(data)

quit

```

The python environment is available to R as `py`. We can thus manipulate the data with the usual functions. In particular, the variable `pupil` is read as an array, whereas we would prefer a long dataframe in R. We thus reshape a bit the data as follows - based on my understanding of the structure:

```{r eval= F}
DF= py$data #bring to R

DF2 = lapply(1:nrow(DF), function(x) {
  res = data.frame(
    Pupil = as.vector(DF$pupil[[x]]),
    Set_size = DF$set_size[x],
    Subject = DF$subject_nr[x],
    Color_type = DF$color_type[x],
    Time = seq(0, 3, length = 300)
  )
  
  return(res)
  
})

DF2= do.call(rbind, DF2)

```

We further add an explicit "Trial" variable to the data. The variable "Set_Size" is also converted to a factor.

```{r eval= F}
DF2= DF2 %>% 
  group_by(Subject, Time) %>% 
  mutate(Trial= 1:n())

DF2= data.frame(DF2)

DF2$Set_size= as.factor(DF2$Set_size)

```

There are a few `NA` in the data. It is generally not problematic for LMEMs, which can handle very well slightly unbalanced designs, and these values could be very well omitted. It is slightly more problematic for the second approach, through features reduction, in that missing values may lead to the loss of an entire trial. For this approach, some sort of interpolation would be warranted if, of course, the gaps are not too large. 
For this vignette we simply interpolate the missing values linearly through `zoo::na.approx()`. Note that this function will not work when NAs are located at the beginning or the end of a vector, so that we won't get rid of all NAs.

```{r echo= F}
DF2= readRDS("data/zhou et al 2022.RData")

```

```{r}
sum(is.na(DF2$Pupil))

DF2= DF2 %>% 
  group_by(Subject, Trial) %>% 
  mutate(Pupil= zoo::na.approx(Pupil, na.rm = F))

sum(is.na(DF2$Pupil))

```

We then plot the average pupil size by Set_Size and Color_type. The plot looks quite like the [original one](https://github.com/smathot/time_series_test), so that we sould not be too far off from the original data.  

```{r mathot plot data, fig.width=8, fig.height=8}
DF2 %>% 
  group_by(Subject, Time, Color_type, Set_size) %>% 
  summarise(Pupil= mean(Pupil, na.rm= T)) %>%
  group_by(Time, Color_type, Set_size) %>% 
  summarise(Pupil= mean(Pupil)) %>%
  mutate(Set_size= as.factor(Set_size)) %>% 
  ggplot(aes(y= Pupil, x= Time, 
             color= Set_size, 
             linetype= Color_type,
             group = interaction(Set_size, Color_type))) +
  geom_line(linewidth= 1.2) +
  theme(text= element_text(size= 16,
                           face="bold")) +
  xlab("Time (s)") +
  ylab("Pupil size (a.u.)") +
  ggtitle("Data from: Zhou, Lorist, and Mathôt (2022)")

```

I am not really depicting the variability. However, what matters here is that: 1) there is a very large effect of Set_Size; 2) there may be (well, we know from the paper that there is, :) ) an interaction between Set_Size and Color_type so that the effect of the non-prototypical color is reversed between 1 and 4. Let's analyze the data!

## Crossvalidated LMEMs

In this approach [Mathôt & Vilotijević, 2022](https://link.springer.com/article/10.3758/s13428-022-01957-7), all trials from each participant are assigned deterministically to one of N folds. N-1 folds are circularly used as the training set; here, LMEMs are performed for each timepoint, and the timepoint having the peak t-value (for each fixed effect or interaction) is then used in the test set to confirm the overall consistency of the target effect across folds. 
This approach is computationally efficient and very powerful in suggesting the presence of a consistent experimental effect *somewhere* along the time course of the trials. 
This is how you would do it in `Pupilla`.

```{r cv, warning= F}
#the data
data= DF2
#lme4::lmer-style formula 
#here we use only random intercepts
formula= "Pupil ~ Set_size*Color_type + (1|Subject)"
#you also have to supply explicitly the names of the variables
dv= "Pupil"
time= "Time"
id= "Subject"
trial= "Trial"
#how many folds?
nfolds= 3
#used for the consensus across folds - see below
t_thresh= 1.9
consensus_thresh = 0.99

cv= decode_signal(data= DF2, 
                  formula= formula,
                  dv= dv,
                  time= time,
                  id= id,
                  trial= trial,
                  nfolds= nfolds,
                  t_thresh= t_thresh,
                  consensus_thresh= consensus_thresh)

```

That's it!
We first take a look at the global effects, with LMEMs tested at the relative peaks:

```{r}
cv$Peaks_test

```

First, you may notice that results are taken from `lmerTest::lmer()` and reflect the default contrast settings of R, that is **treatment coding**. In other words, in the case of factors the first level is taken as the reference. In this case it's pretty clear for us as we aim to compare Set_size 1 vs 4; if you want to compare explicity other contrasts, you would have to refactor the variables. In my understanding other packages may perform "omnibus tests"; in R this is very feasible, e.g. with `car::Anova()`, and perhaps will be implemented in the feature. The interpretation here, however, is not that of an omnibus test (e.g., a main effect in the ANOVA), but that of specific comparisons.   
You will also notice that the main inferential statistic is a t value, rather than a z or F value.

That said, the results are positive for Set_size (each differs from 1) and the interaction with Color_type between Set_size 1 and 4. This would therefore be taken as evidence for the presence of an effect *somewhere* along the time course of trials. 

The following table reports statistics for each fold, *when the fold was left out*. This is useful to understand where the peaks were located.

```{r}
cv$All_Folds

```

First, the comparisons with Set_size 1 vs 2 or 3 are similar in locating peaks around 1 second. Results are slightly more sparse for the 1 vs 4 comparison (1 to 2.8 s).
We did not find a main effect of Color_type here. This could be due to the contrasts used, to differences in how trials are assigned to folds, the presence of possible convergence failures, etc. You will see, for example, that t values for the main effect of Color_type are positive in two folds, negative in the other one. This could be due, again, to some funny convergence, or simply the fact that overfitting occurrred in one specific time point (sample 5). Below we will introduce the notion of **consensus** across folds.
Finally, the interaction is also seemingly very consistent between 1.4 and 1.7 seconds.

Overall, it seems to me that there is a good agreement with the original findings, notwithstanding the differences in the implementation.

There are, however, a couple of caveats, that are also mentioned by [the authors](https://github.com/smathot/time_series_test). 

First, the temporal information provided by this approach is rather coarse. It can test the presence of an effect *somewhere* along the course of the trial, though the specific timepoints may be rather vague.

Second, in the presence of two distinct loci of effects, this approach would not be entirely appropriate, especially if the direction of these effects is in the opposite direction.

Finally, because the tested time point is chosen as the one having the peak statistic (e.g., t value), the approach may be prone to overfitting; other time points may be, in fact, more representative of the effects at hand.

### Consensus

It is a common problem, in machine learning, having to find a consensus between the hyperparameters obtained in different folds. Particularly so if you need to build one final model to interpret the results (see, e.g., my other package [FCnet](https://eblini.github.io/FCnet/articles/FCnet_overview_of_package.html) for the analysis of neuroimaging data through elastic nets). 

Long story short, one possible solution that is put forward in `Pupilla` is quite simple, though arbitrary: we simply keep track of the timepoints that are associated to a given threshold, e.g. a t statistic of 1.9 as in this case, and we consider them to be in agreement across folds if... these time points show up consistently across folds! It's as simple as that. In this specific example we ask for all folds to be in agreement (>99%), meaning that all timepoints must remain significant regardless of which fold is left out from the data. (This is to ensure that none of the folds has particular leverage in driving the overal results).
For the effects that we have specified, was there a consensus across folds?

```{r}
cv$Consensus

```

Yes! The consensus was very clear for each contrast within Set_size **and** for the interaction with Color_type (1 vs 4), in the latter case between sample 136 and 147. We can then formally test the consensus window with another LMEM:

```{r}
cv$Consensus_test

```

From which, again, the presence of an effect is confirmed.

To summarise, this approach has the advantage of returning a more precise time window for the given experimental effect; it maintains the computational efficiency of the crossvalidated approach; it is capable to highlight potentially clusters that are not contiguous.

The downside is that few very arbitrary choices - e.g., the threshold for the statistic - must be specified beforehand. My personal take on the matter is that we, as researchers, should start being more comfortable with our degrees of freedom, which are unavoidable in our job, embrace them, and just resolve to tranparently report our choices and avoid overselling stuff. But I get it, we are not really ingrained to do that. Keep in mind that, however, this conflict can be resolved very elegantly by preregistration of the experimental methods. 
At any rate, this would be too long of a discussion for this vignette. Let's introduce the second approach pursued by `Pupilla`: **features reduction**.

## Feature reduction

We live in a time of big data. As a result, it is often necessary (and very empowering) to reduce the dimensions of the initial data to few, much more manageable features that preserve most of the original variability. For example through Principal components analysis or Independent Components Analysis very rich data (e.g., neuroimaging) can be reduced to very few features, which can then enter multivariate analyses (see, e.g., [FCnet](https://eblini.github.io/FCnet/articles/FCnet_overview_of_package.html)). 
The same can be done with pupillometry data. In fact, because pupil dilation is strongly autocorrelated in time, this approach is potentially very effective. `Pupilla::reduce_PCA()` and `Pupilla::reduce_ICA()` attempt to do that by summarising each trial in very few scores; the scores reflect the contribution of pupil diameter at each time point, weighted by the loadings matrix. The loadings  vary as a function of the importance of the contribution of each time point, thus capturing some sort of relevant pattern in the data. 

The notation to do this in `Pupilla` is very similar to what used above:

```{r fr}
#the data
data= DF2
#variables names should be supplied explicitly 
dv= "Pupil"
time= "Time"
id= "Subject"
trial= "Trial"
#append to the reduced dataframe
add= c("Set_size", "Color_type")

rf = reduce_PCA(
  data = DF2,
  dv = dv,
  time = time,
  id = id,
  trial = trial,
  add = add
)

```

We have used PCA (though ICA would be very similar in this specific case). As we can see the approach is tremendously effective in reducing the data, because only 2 components can explain more than 90% of the original data! This means that we can summarise down each trial and describe fairly well the pattern of pupil dilation with only 2 values!

```{r}
rf$summaryPCA[,1:4]

```

Generally speaking the first component captures the main axis of variability, and ends up describing at best the overall shape of the curves seen in the original plot above. We can depict it as:

```{r}
plot_loadings("PC1", rf)

```

The loadings suggest that the first PC captures an overall dilation/constriction occurring in the late stages of the trial, after 1 second. (Keep in mind that the sign of the loadings is completely arbitrary, this plot could very well be reversed, but we know from the data that it probably refers to dilation.)
Then, the following components gruadually capture the remaining variability, e.g.:

```{r}
plot_loadings("PC2", rf)

```

Meaning that the more you go on, the more you are likely to capture very idiosincratic (and very small) trial-wise changes. It is not really the case for this second component (24% of the variance explained), which captures very well what is likely to be an initial response, e.g. the pupillary light reflex. 
A note about the interpretation: because PCA is a descriptive model, it does not imply any underlying, originating process. The interpretation part is completely up to the experimenter. This would be the case, however, for any inferential technique. And for most uses, the shape of the loadings plot is more likely to highlight segregated, at least partially independent processes.

We move forward by assessing, thus, the obtained scores. Scores can be, of course, summarised per subject to perform classic ANOVAs, or used as such within LMEMs. We choose the second approach for now, though we still summarise the data for depiction purposes.

```{r}
#summarise scores
Scores= rf$Scores %>% 
  group_by(id, Set_size, Color_type) %>% 
  summarise(PC1= mean(PC1),
            PC2= mean(PC2))
Scores$Set_size= as.factor(Scores$Set_size)
rf$Scores$Set_size= as.factor(rf$Scores$Set_size)

```

### PC1

We can fit a LMEM model for the first PC:

```{r}
mod_pc1= lmer(PC1 ~ Color_type*Set_size + (1|id),
              rf$Scores) #note that we used the original dataframe
summary(mod_pc1)

```

To see, again, the large effect of Set_size, and the interaction with Color_type occurring for the 1 vs 4 contrast. With the package `emmeans` we can avoid refactoring to compute all contrasts:

```{r}
emm_options(lmer.df = "asymptotic")
mm= emmeans(mod_pc1, ~ Color_type:Set_size)
mm
contrast(mm, method = "pairwise", interaction = T)

```

This is, indeed, the only significant contrast. We can see that there is an inversion in the effect of prototypical / non-prototypical color between the two set sizes.

```{r}
ggplot(Scores, aes(y= PC1, x= Set_size, fill= Color_type)) +
  geom_boxplot() +
  theme(text= element_text(size= 16,
                           face="bold")) 

```

### PC2

For PC2, on the other hand:

```{r}
mod_pc2= lmer(PC2 ~ Color_type*Set_size + (1|id), 
              rf$Scores)
summary(mod_pc2)

```

We find no such effect, reiterating that the interaction probably appears later on in the course of the trial.

## Discussion

The aim of this vignette was to present the two main analytical approaches of `Pupilla`. I am not familiar enough with the domain of the original study to adventure myself in theoretical interpretations. Suffice to say that, in my opinion, the crossvalidation approach appears to replicate quite well - notwithstanding differences in the implementation - the results reported by the authors. Furthermore, an argument could be made about the utility of a *consensus* across folds to enhance the precision in identifying a temporal cluster.
For what concerns the second approach, i.e. via feature reduction, we have also seen pretty coherent results. I would not go as far as saying that the loadings of PC1 can identify a temporal cluster. In a sense, they would, though not of course a dichotomic one, but rather a much more graded, continuous one. All I care to say here, however, is that this approach could represent a viable option for when the collected data is really huge, in that it would allow one to obtain few, much more manageable variables that can be further exploited for all sorts of modelling purposes. Furthermore, these variables may be more easy to interpret - albeit this is, however, still to better understand with more research.

## Appendix

Packages' versions:

```{r}
sessionInfo()

```


